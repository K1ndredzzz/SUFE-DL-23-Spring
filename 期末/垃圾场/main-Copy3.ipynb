{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bdb99d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-07T16:14:46.679087Z",
     "iopub.status.busy": "2023-06-07T16:14:46.678560Z",
     "iopub.status.idle": "2023-06-07T16:19:28.932338Z",
     "shell.execute_reply": "2023-06-07T16:19:28.931591Z",
     "shell.execute_reply.started": "2023-06-07T16:14:46.678978Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-206d9704fbfa435d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc34cee2d4b43b6a7572ee863779090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97297dface9471085ed6d426a73487b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-206d9704fbfa435d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/2086 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c91eb647d14e4abe67a667a99e3216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90b9e43d4804dffa7f80648c93b7569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a7b6c121494b8081938ce94b1414ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16389c30467d466bbd266cd5c4405b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2086 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa40fd4cb754a80ab50c0a631d37999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 343.7296, Validation Accuracy: 0.0024\n",
      "Epoch: 2, Train Loss: 341.0904, Validation Accuracy: 0.0000\n",
      "Epoch: 3, Train Loss: 338.1317, Validation Accuracy: 0.0048\n",
      "Epoch: 4, Train Loss: 333.0221, Validation Accuracy: 0.0024\n",
      "Epoch: 5, Train Loss: 326.4313, Validation Accuracy: 0.0120\n",
      "Epoch: 6, Train Loss: 316.1377, Validation Accuracy: 0.0311\n",
      "Epoch: 7, Train Loss: 301.0961, Validation Accuracy: 0.0766\n",
      "Epoch: 8, Train Loss: 286.9303, Validation Accuracy: 0.1029\n",
      "Epoch: 9, Train Loss: 269.3817, Validation Accuracy: 0.1699\n",
      "Epoch: 10, Train Loss: 251.5050, Validation Accuracy: 0.2321\n",
      "Epoch: 11, Train Loss: 233.1118, Validation Accuracy: 0.2943\n",
      "Epoch: 12, Train Loss: 213.9955, Validation Accuracy: 0.3254\n",
      "Epoch: 13, Train Loss: 194.3248, Validation Accuracy: 0.3421\n",
      "Epoch: 14, Train Loss: 173.5190, Validation Accuracy: 0.3660\n",
      "Epoch: 15, Train Loss: 152.7261, Validation Accuracy: 0.3971\n",
      "Epoch: 16, Train Loss: 133.8985, Validation Accuracy: 0.4163\n",
      "Epoch: 17, Train Loss: 115.6827, Validation Accuracy: 0.4306\n",
      "Epoch: 18, Train Loss: 99.4820, Validation Accuracy: 0.4617\n",
      "Epoch: 19, Train Loss: 85.6794, Validation Accuracy: 0.4785\n",
      "Epoch: 20, Train Loss: 72.5879, Validation Accuracy: 0.4856\n",
      "Epoch: 21, Train Loss: 61.6959, Validation Accuracy: 0.5167\n",
      "Epoch: 22, Train Loss: 53.0808, Validation Accuracy: 0.5383\n",
      "Epoch: 23, Train Loss: 46.0860, Validation Accuracy: 0.5550\n",
      "Epoch: 24, Train Loss: 39.2399, Validation Accuracy: 0.5502\n",
      "Epoch: 25, Train Loss: 34.3892, Validation Accuracy: 0.5646\n",
      "Epoch: 26, Train Loss: 30.3117, Validation Accuracy: 0.5670\n",
      "Epoch: 27, Train Loss: 26.9939, Validation Accuracy: 0.5718\n",
      "Epoch: 28, Train Loss: 24.2234, Validation Accuracy: 0.5957\n",
      "Epoch: 29, Train Loss: 21.9567, Validation Accuracy: 0.5885\n",
      "Epoch: 30, Train Loss: 19.9066, Validation Accuracy: 0.5957\n",
      "Epoch: 31, Train Loss: 18.2882, Validation Accuracy: 0.5981\n",
      "Epoch: 32, Train Loss: 17.1869, Validation Accuracy: 0.6124\n",
      "Epoch: 33, Train Loss: 16.2025, Validation Accuracy: 0.6220\n",
      "Epoch: 34, Train Loss: 15.0820, Validation Accuracy: 0.6196\n",
      "Epoch: 35, Train Loss: 14.0630, Validation Accuracy: 0.6124\n",
      "Epoch: 36, Train Loss: 13.4402, Validation Accuracy: 0.6196\n",
      "Epoch: 37, Train Loss: 12.8517, Validation Accuracy: 0.6292\n",
      "Epoch: 38, Train Loss: 12.3638, Validation Accuracy: 0.6268\n",
      "Epoch: 39, Train Loss: 11.8605, Validation Accuracy: 0.6196\n",
      "Epoch: 40, Train Loss: 11.6863, Validation Accuracy: 0.6292\n",
      "Epoch: 41, Train Loss: 11.3111, Validation Accuracy: 0.6316\n",
      "Epoch: 42, Train Loss: 11.0763, Validation Accuracy: 0.6292\n",
      "Epoch: 43, Train Loss: 10.8359, Validation Accuracy: 0.6316\n",
      "Epoch: 44, Train Loss: 10.7635, Validation Accuracy: 0.6316\n",
      "Epoch: 45, Train Loss: 10.5969, Validation Accuracy: 0.6316\n",
      "Epoch: 46, Train Loss: 10.5544, Validation Accuracy: 0.6340\n",
      "Epoch: 47, Train Loss: 10.6948, Validation Accuracy: 0.6340\n",
      "Epoch: 48, Train Loss: 10.5931, Validation Accuracy: 0.6316\n",
      "Epoch: 49, Train Loss: 10.5314, Validation Accuracy: 0.6316\n",
      "Epoch: 50, Train Loss: 10.6652, Validation Accuracy: 0.6316\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-67caf77471a5a48e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bb430fd431432a8e6fc6a830a43bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e22836245944ef87b245cdf7650297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-67caf77471a5a48e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1423 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, set_seed\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 定义超参数\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "MAX_LENGTH = 35  # 最大长度 query句子长度大多不超过35\n",
    "LR = 5e-5\n",
    "TRAIN_SIZE = 0.8  # 训练集比例\n",
    "# set_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "# 加载csv文件作为训练集\n",
    "raw_dataset = load_dataset('csv', data_files='train_dataset.csv', split='train')\n",
    "# 对标签进行编码 将babel列向量数字编码\n",
    "raw_dataset = raw_dataset.class_encode_column('label')\n",
    "# 加载模型bert中文预训练模型\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "# 定义分词器函数 对query列文本进行编码\n",
    "def tokenize_function(examples): # 对query列 超过max_length截断 少于的补零\n",
    "    return tokenizer(examples['query'], truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
    "\n",
    "# 应用分词函数编码应用分词函数编码到整个数据集 分batch 去掉#列即序号\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['#'])\n",
    "\n",
    "# 训练集和验证集划分 以观测训练效果\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(train_size=TRAIN_SIZE, seed=42)\n",
    "train_dataset = tokenized_dataset['train']\n",
    "valid_dataset = tokenized_dataset['test']\n",
    "\n",
    "# 设置该三列数据格式为pytorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "valid_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# 数据加载 训练集shuffle 验证集可以不shuffle\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# 模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=train_dataset.features['label'].num_classes)\n",
    "model.to('cuda')\n",
    "# 优化器\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "# lr scheduler 自动调整LR 定义LR上限 每轮训练步骤数\n",
    "scheduler = OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
    "\n",
    "\n",
    "\n",
    "# 训练\n",
    "best_acc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        labels = batch['label'].to('cuda')\n",
    "\n",
    "        # 前向传播算loss 反向传播更新梯度\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # 更新参数 更新学习率 清空梯度\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    # 评估模式 计算验证集精度\n",
    "    model.eval()\n",
    "    valid_predictions = []\n",
    "    for batch in valid_dataloader:\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        #收集预测结果\n",
    "        valid_predictions.extend(outputs.logits.argmax(-1).cpu().numpy())\n",
    "    \n",
    "    valid_acc = np.mean(valid_predictions == valid_dataset['label'].numpy())\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {total_loss:.4f}, Validation Accuracy: {valid_acc:.4f}\")\n",
    "    \n",
    "    # 保存在验证集上最好的结果\n",
    "    if valid_acc >= best_acc:\n",
    "        model.save_pretrained('output')\n",
    "\n",
    "\n",
    "\n",
    "# 预测结果\n",
    "# 加载测试集 保证编码与训练时一致\n",
    "test_dataset = load_dataset('csv', data_files='test_dataset.csv', split='train')\n",
    "test_id = test_dataset['id']\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns='id')\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 调用模型训练好的参数\n",
    "model = AutoModelForSequenceClassification.from_pretrained('output')\n",
    "model.to('cuda')\n",
    "\n",
    "# 评估模式 预测结果\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids'].to('cuda')\n",
    "    attention_mask = batch['attention_mask'].to('cuda')\n",
    "    # 禁用梯度\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    predictions.extend(outputs.logits.argmax(-1).cpu().numpy())\n",
    "\n",
    "# 向量转字符串\n",
    "predicted_labels = train_dataset.features['label'].int2str(predictions)\n",
    "\n",
    "# 下载结果csv文件\n",
    "submission = pd.DataFrame({'id': test_id, 'label': predicted_labels})\n",
    "submission.to_csv('submit_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ed627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
