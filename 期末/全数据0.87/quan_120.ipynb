{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bdb99d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T09:26:07.852858Z",
     "iopub.status.busy": "2023-06-09T09:26:07.852486Z",
     "iopub.status.idle": "2023-06-09T09:46:50.383719Z",
     "shell.execute_reply": "2023-06-09T09:46:50.382862Z",
     "shell.execute_reply.started": "2023-06-09T09:26:07.852790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-2de56802f1ede12a/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-2de56802f1ede12a/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-87761e9464147f5e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-2de56802f1ede12a/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-08941be77ca52344.arrow\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 543.007, Validation Accuracy: 0.0023\n",
      "Epoch: 2, Train Loss: 538.488, Validation Accuracy: 0.0034\n",
      "Epoch: 3, Train Loss: 533.598, Validation Accuracy: 0.0064\n",
      "Epoch: 4, Train Loss: 526.835, Validation Accuracy: 0.0143\n",
      "Epoch: 5, Train Loss: 519.426, Validation Accuracy: 0.0312\n",
      "Epoch: 6, Train Loss: 510.140, Validation Accuracy: 0.0564\n",
      "Epoch: 7, Train Loss: 500.138, Validation Accuracy: 0.1053\n",
      "Epoch: 8, Train Loss: 487.001, Validation Accuracy: 0.1888\n",
      "Epoch: 9, Train Loss: 474.336, Validation Accuracy: 0.3042\n",
      "Epoch: 10, Train Loss: 460.132, Validation Accuracy: 0.4411\n",
      "Epoch: 11, Train Loss: 443.205, Validation Accuracy: 0.5513\n",
      "Epoch: 12, Train Loss: 426.586, Validation Accuracy: 0.6706\n",
      "Epoch: 13, Train Loss: 406.720, Validation Accuracy: 0.7533\n",
      "Epoch: 14, Train Loss: 386.841, Validation Accuracy: 0.8221\n",
      "Epoch: 15, Train Loss: 366.393, Validation Accuracy: 0.8740\n",
      "Epoch: 16, Train Loss: 343.624, Validation Accuracy: 0.9007\n",
      "Epoch: 17, Train Loss: 318.455, Validation Accuracy: 0.9203\n",
      "Epoch: 18, Train Loss: 293.062, Validation Accuracy: 0.9440\n",
      "Epoch: 19, Train Loss: 268.873, Validation Accuracy: 0.9481\n",
      "Epoch: 20, Train Loss: 242.472, Validation Accuracy: 0.9639\n",
      "Epoch: 21, Train Loss: 214.583, Validation Accuracy: 0.9684\n",
      "Epoch: 22, Train Loss: 187.941, Validation Accuracy: 0.9782\n",
      "Epoch: 23, Train Loss: 163.702, Validation Accuracy: 0.9808\n",
      "Epoch: 24, Train Loss: 138.912, Validation Accuracy: 0.9838\n",
      "Epoch: 25, Train Loss: 115.318, Validation Accuracy: 0.9872\n",
      "Epoch: 26, Train Loss: 96.315, Validation Accuracy: 0.9883\n",
      "Epoch: 27, Train Loss: 79.594, Validation Accuracy: 0.9936\n",
      "Epoch: 28, Train Loss: 64.635, Validation Accuracy: 0.9936\n",
      "Epoch: 29, Train Loss: 53.127, Validation Accuracy: 0.9959\n",
      "Epoch: 30, Train Loss: 43.471, Validation Accuracy: 0.9966\n",
      "Epoch: 31, Train Loss: 37.199, Validation Accuracy: 0.9970\n",
      "Epoch: 32, Train Loss: 30.107, Validation Accuracy: 0.9959\n",
      "Epoch: 33, Train Loss: 25.327, Validation Accuracy: 0.9966\n",
      "Epoch: 34, Train Loss: 21.447, Validation Accuracy: 0.9955\n",
      "Epoch: 35, Train Loss: 18.443, Validation Accuracy: 0.9981\n",
      "Epoch: 36, Train Loss: 16.176, Validation Accuracy: 0.9974\n",
      "Epoch: 37, Train Loss: 13.705, Validation Accuracy: 0.9985\n",
      "Epoch: 38, Train Loss: 12.176, Validation Accuracy: 0.9981\n",
      "Epoch: 39, Train Loss: 10.998, Validation Accuracy: 0.9985\n",
      "Epoch: 40, Train Loss: 9.591, Validation Accuracy: 0.9981\n",
      "Epoch: 41, Train Loss: 8.876, Validation Accuracy: 0.9977\n",
      "Epoch: 42, Train Loss: 8.134, Validation Accuracy: 0.9985\n",
      "Epoch: 43, Train Loss: 7.353, Validation Accuracy: 0.9981\n",
      "Epoch: 44, Train Loss: 6.780, Validation Accuracy: 0.9985\n",
      "Epoch: 45, Train Loss: 6.086, Validation Accuracy: 0.9985\n",
      "Epoch: 46, Train Loss: 5.641, Validation Accuracy: 0.9985\n",
      "Epoch: 47, Train Loss: 5.382, Validation Accuracy: 0.9985\n",
      "Epoch: 48, Train Loss: 5.327, Validation Accuracy: 0.9981\n",
      "Epoch: 49, Train Loss: 4.785, Validation Accuracy: 0.9985\n",
      "Epoch: 50, Train Loss: 4.286, Validation Accuracy: 0.9985\n",
      "Epoch: 51, Train Loss: 3.986, Validation Accuracy: 0.9985\n",
      "Epoch: 52, Train Loss: 3.739, Validation Accuracy: 0.9985\n",
      "Epoch: 53, Train Loss: 3.564, Validation Accuracy: 0.9985\n",
      "Epoch: 54, Train Loss: 3.360, Validation Accuracy: 0.9985\n",
      "Epoch: 55, Train Loss: 3.142, Validation Accuracy: 0.9985\n",
      "Epoch: 56, Train Loss: 2.995, Validation Accuracy: 0.9985\n",
      "Epoch: 57, Train Loss: 2.891, Validation Accuracy: 0.9985\n",
      "Epoch: 58, Train Loss: 2.691, Validation Accuracy: 0.9985\n",
      "Epoch: 59, Train Loss: 2.560, Validation Accuracy: 0.9985\n",
      "Epoch: 60, Train Loss: 2.489, Validation Accuracy: 0.9985\n",
      "Epoch: 61, Train Loss: 2.372, Validation Accuracy: 0.9985\n",
      "Epoch: 62, Train Loss: 2.243, Validation Accuracy: 0.9985\n",
      "Epoch: 63, Train Loss: 2.182, Validation Accuracy: 0.9985\n",
      "Epoch: 64, Train Loss: 2.039, Validation Accuracy: 0.9985\n",
      "Epoch: 65, Train Loss: 1.990, Validation Accuracy: 0.9985\n",
      "Epoch: 66, Train Loss: 1.924, Validation Accuracy: 0.9985\n",
      "Epoch: 67, Train Loss: 1.850, Validation Accuracy: 0.9985\n",
      "Epoch: 68, Train Loss: 1.779, Validation Accuracy: 0.9985\n",
      "Epoch: 69, Train Loss: 1.758, Validation Accuracy: 0.9985\n",
      "Epoch: 70, Train Loss: 2.056, Validation Accuracy: 0.9985\n",
      "Epoch: 71, Train Loss: 1.631, Validation Accuracy: 0.9985\n",
      "Epoch: 72, Train Loss: 1.649, Validation Accuracy: 0.9985\n",
      "Epoch: 73, Train Loss: 1.555, Validation Accuracy: 0.9985\n",
      "Epoch: 74, Train Loss: 1.490, Validation Accuracy: 0.9985\n",
      "Epoch: 75, Train Loss: 1.713, Validation Accuracy: 0.9985\n",
      "Epoch: 76, Train Loss: 1.544, Validation Accuracy: 0.9985\n",
      "Epoch: 77, Train Loss: 1.398, Validation Accuracy: 0.9985\n",
      "Epoch: 78, Train Loss: 1.386, Validation Accuracy: 0.9985\n",
      "Epoch: 79, Train Loss: 1.294, Validation Accuracy: 0.9985\n",
      "Epoch: 80, Train Loss: 1.213, Validation Accuracy: 0.9985\n",
      "Epoch: 81, Train Loss: 1.169, Validation Accuracy: 0.9985\n",
      "Epoch: 82, Train Loss: 1.159, Validation Accuracy: 0.9985\n",
      "Epoch: 83, Train Loss: 1.116, Validation Accuracy: 0.9985\n",
      "Epoch: 84, Train Loss: 1.114, Validation Accuracy: 0.9985\n",
      "Epoch: 85, Train Loss: 1.045, Validation Accuracy: 0.9985\n",
      "Epoch: 86, Train Loss: 1.042, Validation Accuracy: 0.9985\n",
      "Epoch: 87, Train Loss: 1.027, Validation Accuracy: 0.9985\n",
      "Epoch: 88, Train Loss: 1.015, Validation Accuracy: 0.9985\n",
      "Epoch: 89, Train Loss: 1.002, Validation Accuracy: 0.9985\n",
      "Epoch: 90, Train Loss: 0.959, Validation Accuracy: 0.9985\n",
      "Epoch: 91, Train Loss: 0.921, Validation Accuracy: 0.9985\n",
      "Epoch: 92, Train Loss: 0.941, Validation Accuracy: 0.9985\n",
      "Epoch: 93, Train Loss: 1.225, Validation Accuracy: 0.9985\n",
      "Epoch: 94, Train Loss: 1.092, Validation Accuracy: 0.9985\n",
      "Epoch: 95, Train Loss: 0.949, Validation Accuracy: 0.9985\n",
      "Epoch: 96, Train Loss: 0.912, Validation Accuracy: 0.9985\n",
      "Epoch: 97, Train Loss: 0.878, Validation Accuracy: 0.9985\n",
      "Epoch: 98, Train Loss: 0.867, Validation Accuracy: 0.9985\n",
      "Epoch: 99, Train Loss: 0.850, Validation Accuracy: 0.9985\n",
      "Epoch: 100, Train Loss: 0.854, Validation Accuracy: 0.9985\n",
      "Epoch: 101, Train Loss: 0.830, Validation Accuracy: 0.9985\n",
      "Epoch: 102, Train Loss: 0.804, Validation Accuracy: 0.9985\n",
      "Epoch: 103, Train Loss: 0.808, Validation Accuracy: 0.9985\n",
      "Epoch: 104, Train Loss: 0.818, Validation Accuracy: 0.9985\n",
      "Epoch: 105, Train Loss: 0.795, Validation Accuracy: 0.9985\n",
      "Epoch: 106, Train Loss: 0.777, Validation Accuracy: 0.9985\n",
      "Epoch: 107, Train Loss: 0.783, Validation Accuracy: 0.9985\n",
      "Epoch: 108, Train Loss: 0.789, Validation Accuracy: 0.9985\n",
      "Epoch: 109, Train Loss: 0.775, Validation Accuracy: 0.9985\n",
      "Epoch: 110, Train Loss: 0.747, Validation Accuracy: 0.9985\n",
      "Epoch: 111, Train Loss: 0.763, Validation Accuracy: 0.9985\n",
      "Epoch: 112, Train Loss: 0.747, Validation Accuracy: 0.9985\n",
      "Epoch: 113, Train Loss: 0.761, Validation Accuracy: 0.9985\n",
      "Epoch: 114, Train Loss: 0.768, Validation Accuracy: 0.9985\n",
      "Epoch: 115, Train Loss: 0.763, Validation Accuracy: 0.9985\n",
      "Epoch: 116, Train Loss: 0.739, Validation Accuracy: 0.9985\n",
      "Epoch: 117, Train Loss: 0.737, Validation Accuracy: 0.9985\n",
      "Epoch: 118, Train Loss: 0.746, Validation Accuracy: 0.9985\n",
      "Epoch: 119, Train Loss: 0.753, Validation Accuracy: 0.9985\n",
      "Epoch: 120, Train Loss: 0.755, Validation Accuracy: 0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-67caf77471a5a48e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-67caf77471a5a48e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-dcc4d91e0724ffa4.arrow\n"
     ]
    }
   ],
   "source": [
    "# 引入所需的库和模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, set_seed\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 定义超参数\n",
    "BATCH_SIZE = 32  # 批处理大小\n",
    "EPOCHS = 120  # 迭代次数\n",
    "MAX_LENGTH = 40  # 序列最大长度\n",
    "LEARNING_RATE = 5e-5  # 学习率\n",
    "WEIGHT_DECAY = 0.01  # 权重衰减系数\n",
    "NUM_WARMUP_STEPS = 1000  # 热身步骤数\n",
    "MODEL_NAME = 'bert-base-chinese'  # 预训练模型名\n",
    "\n",
    "# 设置随机种子，保证结果可复现\n",
    "set_seed(42)\n",
    "\n",
    "# 加载csv数据集，数据路径为'train_dataset.csv'\n",
    "raw_dataset = load_dataset('csv', data_files='train_dataset.csv', split='train')\n",
    "\n",
    "# 对标签进行编码，将类别标签转化为数字\n",
    "raw_dataset = raw_dataset.class_encode_column('label')\n",
    "\n",
    "# 从预训练模型名加载Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 定义编码函数，对输入的文本进行编码\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['query'], truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
    "\n",
    "# 对数据集进行编码\n",
    "raw_dataset = raw_dataset.map(encode, batched=True, remove_columns='#')\n",
    "\n",
    "\n",
    "# # 不划分训练集于验证集（需要注释下面几行代码）\n",
    "train_dataset = raw_dataset\n",
    "valid_dataset = raw_dataset\n",
    "\n",
    "# 划分训练集和验证集\n",
    "# raw_dataset = raw_dataset.train_test_split(train_size=TRAIN_SIZE, seed=42)\n",
    "# train_dataset = raw_dataset['train']\n",
    "# valid_dataset = raw_dataset['test']\n",
    "\n",
    "\n",
    "# 设置数据格式，用于训练模型\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "valid_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 准备模型，优化器和学习率调度器\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=train_dataset.features['label'].num_classes)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
    "\n",
    "# 进行模型训练\n",
    "best_acc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # 模型前向传播，计算loss\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        # 反向传播，更新梯度\n",
    "        loss.backward()\n",
    "\n",
    "        # 优化器步进，更新模型参数\n",
    "        optimizer.step()\n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "    # 验证模型，计算验证集上的精度\n",
    "    model.eval()\n",
    "    valid_predictions = []\n",
    "    for batch in valid_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        valid_predictions.extend(outputs.logits.argmax(-1).cpu().numpy())\n",
    "\n",
    "    valid_acc = np.mean(valid_predictions == valid_dataset['label'].numpy())\n",
    "    print(f\"Epoch: {epoch + 1}, Train Loss: {total_loss:.3f}, Validation Accuracy: {valid_acc:.4f}\")\n",
    "\n",
    "    # 保存在验证集上最好的结果\n",
    "    if valid_acc >= best_acc:\n",
    "        model.save_pretrained('output')\n",
    "\n",
    "\n",
    "# 加载测试数据集，对测试集进行预测\n",
    "test_dataset = load_dataset('csv', data_files='test_dataset.csv', split='train')\n",
    "test_id = test_dataset['id']\n",
    "test_dataset = test_dataset.map(encode, batched=True, remove_columns='id')\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('output')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    predictions.extend(outputs.logits.argmax(-1).cpu().numpy())\n",
    "\n",
    "# 将预测的标签从数字转换回原始的类别\n",
    "predicted_labels = train_dataset.features['label'].int2str(predictions)\n",
    "\n",
    "# 保存预测结果，以csv格式存储\n",
    "submission = pd.DataFrame({'id': test_id, 'label': predicted_labels})\n",
    "submission.to_csv('submit_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a350a-918c-4c34-93ba-b0aa68a442b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
